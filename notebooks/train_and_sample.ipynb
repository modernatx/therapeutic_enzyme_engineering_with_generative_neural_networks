{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from therapeutic_enzyme_engineering_with_generative_neural_networks.models import VAE\n",
    "from therapeutic_enzyme_engineering_with_generative_neural_networks.SeqLikeDataset import SeqLikeDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "from Bio.SeqIO import parse\n",
    "from seqlike import aaSeqLike\n",
    "from seqlike.alphabets import AA\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "import seaborn as sns; sns.set_style(\"ticks\")\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['image.interpolation'] = 'nearest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following file will exist if you've run the included get_homologous_seqs and remove_gappy_seqs notebooks.\n",
    "input_file = '../data/tr-B5LY47-B5LY47-ECOLX_blast_nr_5000_aligned.fasta'\n",
    "seqs = pd.Series([aaSeqLike(s, alphabet=AA) for s in parse(input_file, 'fasta')])\n",
    "seqs = pd.Series([x for x in seqs if 'B' not in x and 'X' not in x])\n",
    "\n",
    "dataset = SeqLikeDataset(seqs)\n",
    "\n",
    "train_size = int(len(dataset)*0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=128)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs.sample(10).seq.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and optimizer creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = VAE(sequence_length=seqs.apply(len).max(), layer_sizes=[128, 96, 64], z_size=64).to(device)\n",
    "v.losses = []\n",
    "\n",
    "vae_encoder_optimizer = torch.optim.Adam(v.encoder.parameters(), amsgrad=True)\n",
    "vae_decoder_optimizer = torch.optim.Adam(v.decoder.parameters(), amsgrad=True)\n",
    "\n",
    "batch, _ = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = 0\n",
    "outer = tqdm(range(30))\n",
    "for epoch in outer:\n",
    "    inner = tqdm(train_loader, leave=False)\n",
    "    v.train()\n",
    "\n",
    "    for i, (data, target) in enumerate(inner):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        vae_encoder_optimizer.zero_grad()\n",
    "        vae_decoder_optimizer.zero_grad()\n",
    "\n",
    "        recon, mu, logvar, z_sample = v(data)\n",
    "\n",
    "        # recon loss\n",
    "        recon_loss = -(data * torch.log(recon) + (1 - data) * torch.log(1 - recon)).squeeze().mean(dim=1).mean(dim=1)\n",
    "\n",
    "        # KL loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        kl_element = 1 + logvar - mu ** 2 - logvar.exp()\n",
    "        kl_loss = -0.5 * kl_element.mean(dim=1) * (batches + 1) / 1300.0\n",
    "\n",
    "        total_loss = torch.mean(recon_loss + kl_loss)\n",
    "\n",
    "        v.losses.append(\n",
    "            {\n",
    "                \"batch_id\": batches,\n",
    "                \"train_kl\": kl_loss.data.mean(),\n",
    "                \"train_recon\": recon_loss.data.mean(),\n",
    "                \"train_total\": total_loss.item(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(v.parameters(), 2)\n",
    "        vae_encoder_optimizer.step()\n",
    "        vae_decoder_optimizer.step()\n",
    "\n",
    "        inner.set_description(\n",
    "            \"KL[{:02.6f}], recon=[{:02.6f}], total=[{:02.6f}]\".format(\n",
    "                kl_loss.data.mean(), recon_loss.data.mean(), total_loss.item()\n",
    "            )\n",
    "        )\n",
    "\n",
    "        batches += 1\n",
    "\n",
    "    v.eval()\n",
    "    test_data, test_target = next(iter(test_loader))\n",
    "    test_data = test_data.to(device)\n",
    "    test_target = test_target.to(device)\n",
    "\n",
    "    test_out, mu, logvar, z_sample = v(test_data)\n",
    "\n",
    "    # recon loss\n",
    "    recon_loss = -(data * torch.log(recon) + (1 - data) * torch.log(1 - recon)).squeeze().mean(dim=1).mean(dim=1)\n",
    "\n",
    "    # KL loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    kl_element = 1 + logvar - mu ** 2 - logvar.exp()\n",
    "    kl_loss = -0.5 * kl_element.mean(dim=1) * (batches + 1) / 1300.0\n",
    "\n",
    "    total_loss = torch.mean(recon_loss + kl_loss)\n",
    "\n",
    "    v.losses.append(\n",
    "        {\n",
    "            \"batch_id\": batches,\n",
    "            \"test_total\": total_loss.item(),\n",
    "            \"test_kl\": kl_loss.mean().item(),\n",
    "            \"test_recon\": recon_loss.mean().item(),\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "80cd92a40d98bf380c5491703e7d792ed3c464865a71ab065f4f20e29b2416c5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('therapeutic_enzyme_engineering_with_generative_neural_networks': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
